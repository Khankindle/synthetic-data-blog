{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":58113,"status":"ok","timestamp":1705772170944,"user":{"displayName":"Moritz Laurer","userId":"08772285247446402554"},"user_tz":-60},"id":"lvyHa31FD3Np","outputId":"d1184d0b-d1e6-434c-dd1a-a96cf32773dd"},"outputs":[{"data":{"text/plain":["'%%bash\\npip install --upgrade pip -q\\npip install transformers~=4.37.2\\npip install huggingface_hub~=0.20.3\\npip install datasets~=2.16.1\\npip install openai~=1.11.0\\npip install scikit-learn\\npip install pandas\\npip install tqdm\\npip install python-dotenv'"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# install requirements\n","\"\"\"%%bash\n","pip install --upgrade pip -q\n","pip install transformers~=4.37.2\n","pip install huggingface_hub~=0.20.3\n","pip install datasets~=2.16.1\n","pip install openai~=1.11.0\n","pip install scikit-learn\n","pip install pandas\n","pip install tqdm\n","pip install python-dotenv\"\"\"\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1705772170944,"user":{"displayName":"Moritz Laurer","userId":"08772285247446402554"},"user_tz":-60},"id":"OcuBud1MtgQ9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Notebook running\n"]}],"source":["import os\n","from tqdm import tqdm\n","import ast\n","import numpy as np\n","import pandas as pd\n","import random\n","import json\n","from datetime import datetime\n","\n","from datasets import load_dataset\n","from datasets import concatenate_datasets\n","from openai import OpenAI, AsyncOpenAI\n","import openai\n","\n","import asyncio\n","from aiohttp import ClientSession, ClientTimeout, ClientError\n","from tqdm import tqdm\n","import random\n","import logging\n","\n","print(\"Notebook running\")"]},{"cell_type":"markdown","metadata":{"id":"AGbwNfXdTSlq"},"source":["### Global variables"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":1285,"status":"ok","timestamp":1705772172226,"user":{"displayName":"Moritz Laurer","userId":"08772285247446402554"},"user_tz":-60},"id":"lMeXv8BYs-Dg"},"outputs":[],"source":["# Access the API tokens in .env file\n","from dotenv import load_dotenv\n","load_dotenv()\n","\n","# global variables for APIs\n","HF_TOKEN = os.getenv('HF_TOKEN')\n","API_URL = \"https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1\"\n","HEADERS = {\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n","client_oai = AsyncOpenAI(api_key=os.getenv('OAI_TOKEN'))\n","MODEL = \"gpt-4-0125-preview\"  #\"gpt-3.5-turbo-0125\", \"gpt-4-0125-preview\", \"mixtral\"\n","\n","# choose one of these API providers: \"HF\" or \"OAI\"\n","API_PROVIDER = \"OAI\"\n","# for asynchronous API calls\n","BATCH_SIZE = 64\n","SLEEP_TIME = 1\n","\n","# global variables for experiment variations\n","SEED = 42\n","N_SAMPLE = False  # You can sample parts of the data for faster testing. False for run on full dataset, int for sampling\n","SELF_CONSISTENCY_ITERATIONS = 3  # How many times should the model try to predict the same text for self-consistency?\n","DATA_SUBSET = \"sentences_allagree\"  # \"sentences_allagree\", \"sentences_66agree\", \"sentences_75agree\"\n","FINAL_TEST_RUN = True  # True for final run on test set\n","SAVE_OUTPUTS = True\n"]},{"cell_type":"markdown","metadata":{"id":"PNA50UZUSyZU"},"source":["### Load and prepare dataset"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['sentence', 'label', 'idx'],\n","    num_rows: 453\n","})\n"]}],"source":["# financial_phrasebank paper: https://arxiv.org/pdf/1307.5336.pdf\n","random.seed(SEED)\n","\n","# load dataset\n","dataset = load_dataset(\n","    \"financial_phrasebank\", DATA_SUBSET, \n","    split=\"train\"  # note that the dataset does not have a default test split\n",")\n","\n","# sample for faster testing\n","if N_SAMPLE: \n","    dataset = dataset.select(random.sample(range(len(dataset)), N_SAMPLE))\n","\n","# train-test-split\n","# note: with 0-shot prompting you can sometimes skip holding out a test-set, because you don't train a model\n","# But: the prompt is a form of hyperparameter and every time to adapt the prompt \n","# to get better performance this is a form of hyperparameter search and you do not know how well this prompt would generalize to unseen data.\n","# If you update your prompt, it is therefore good practice to do the final test on a separate test-set\n","# on which the \"prompt wording hyperparameter\" was not tested to avoid overfitting your prompt to the data. \n","# Moreover: for our example, we need a separate testset because we will also train a small BERT model on the training data\n","\n","dataset = dataset.add_column(\"idx\", range(len(dataset)))\n","dataset = dataset.train_test_split(test_size=0.2, shuffle=True, stratify_by_column=\"label\", seed=SEED)\n","\n","# determine ids for train and test set to split again after inference\n","row_id_train = dataset[\"train\"][\"idx\"] \n","row_id_test = dataset[\"test\"][\"idx\"] \n","\n","if FINAL_TEST_RUN and (API_PROVIDER != \"OAI\"):\n","    # merging splits again here for easier inference. Splitting again after inference based on row_ids\n","    dataset = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]])\n","elif API_PROVIDER == \"OAI\":\n","    # for the run with OpenAI models, we only want labels for the testset to calculate metrics\n","    dataset = dataset[\"test\"]\n","else:\n","    # for testing prompts\n","    dataset = dataset[\"train\"]\n","\n","print(dataset)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7c8378f8026f4b8794d923f60d53ec8e","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/453 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['sentence', 'label', 'idx', 'label_text'],\n","    num_rows: 453\n","})\n"]}],"source":["# create a new column with the numeric label verbalised as label_text (e.g. \"positive\" instead of \"0\")\n","label_map = {i: label_text for i, label_text in enumerate(dataset.features[\"label\"].names)}\n","\n","def add_label_text(example):\n","    example[\"label_text\"] = label_map[example[\"label\"]]\n","    return example\n","\n","dataset = dataset.map(add_label_text)\n","\n","print(dataset)"]},{"cell_type":"markdown","metadata":{"id":"xaX6xLzVS1C3"},"source":["### Prompts / Instructions"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1705774111002,"user":{"displayName":"Moritz Laurer","userId":"08772285247446402554"},"user_tz":-60},"id":"8ILH8E7IG9wq"},"outputs":[],"source":["# prompt is inspired by the annotator instructions provided in section \"Annotation task and instructions\"\n","# in the financial_phrasebank paper: https://arxiv.org/pdf/1307.5336.pdf\n","\n","prompt_financial_sentiment = \"\"\"\\\n","You are a highly qualified expert trained to annotate machine learning training data.\n","\n","Your task is to analyze the sentiment in the TEXT below from an investor perspective and label it with only one the three labels:\n","positive, negative, or neutral.\n","\n","Base your label decision only on the TEXT and do not speculate e.g. based on prior knowledge about a company. \n","\n","Do not provide any explanations and only respond with one of the labels as one word: negative, positive, or neutral\n","\n","Examples:\n","Text: Operating profit increased, from EUR 7m to 9m compared to the previous reporting period.\n","Label: positive\n","Text: The company generated net sales of 11.3 million euro this year.\n","Label: neutral\n","Text: Profit before taxes decreased to EUR 14m, compared to EUR 19m in the previous period.\t\n","Label: negative\n","\n","Your TEXT to analyse:\n","TEXT: {text}\n","Label: \"\"\"\n","\n","\n","prompt_financial_sentiment_cot = \"\"\"\\\n","You are a highly qualified expert trained to annotate machine learning training data.\n","\n","Your task is to briefly analyze the sentiment in the TEXT below from an investor perspective and then label it with only one the three labels:\n","positive, negative, neutral.\n","\n","Base your label decision only on the TEXT and do not speculate e.g. based on prior knowledge about a company. \n","\n","You first reason step by step about the correct label and then return your label.\n","\n","You ALWAYS respond only in the following JSON format: {{\"reason\": \"...\", \"label\": \"...\"}}\n","You only respond with one single JSON response. \n","\n","Examples:\n","Text: Operating profit increased, from EUR 7m to 9m compared to the previous reporting period.\n","JSON response: {{\"reason\": \"An increase in operating profit is positive for investors\", \"label\": \"positive\"}}\n","Text: The company generated net sales of 11.3 million euro this year.\n","JSON response: {{\"reason\": \"The text only mentions financials without indication if they are better or worse than before\", \"label\": \"neutral\"}}\n","Text: Profit before taxes decreased to EUR 14m, compared to EUR 19m in the previous period.\t\n","JSON response: {{\"reason\": \"A decrease in profit is negative for investors\", \"label\": \"negative\"}}\n","\n","Your TEXT to analyse:\n","TEXT: {text}\n","JSON response: \"\"\"\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Generation"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# params for API: https://huggingface.co/docs/api-inference/detailed_parameters#text-generation-task\n","# alternative list for API: https://huggingface.github.io/text-generation-inference/#/Text%20Generation%20Inference/generate\n","# params for endpoints: https://huggingface.co/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient\n","\n","generation_params = dict(\n","    top_p=0.90,\n","    top_k=None,\n","    temperature=0.8,\n","    repetition_penalty=1.0,\n","    do_sample=True,\n","    max_new_tokens=128,\n","    return_full_text=False,\n","    #seed=SEED,  # no seed, because we need randomness for self-consistency\n","    max_time=None, \n","    stream=False,\n","    details=False,\n","    use_cache=False,\n","    wait_for_model=False,\n",")"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# asynchronous functions for efficiently calling on LLM APIs with batching\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","\n","\n","# functions for calling the HF API with retries, async and batch processing\n","async def request_with_retry_hf(session, url, headers, json, semaphore, retries=4, backoff_factor=3):\n","    \"\"\"Attempt a request with exponential backoff and retry logic.\"\"\"\n","    attempt = 0\n","    while attempt < retries:\n","        async with semaphore:\n","            try:\n","                async with session.post(url, headers=headers, json=json) as response:\n","                    if response.status in [200, 201]:\n","                        return await response.json()\n","                    elif response.status == 429:\n","                        retry_after = int(response.headers.get(\"Retry-After\", 60))\n","                        logging.warning(f\"Rate limit exceeded. Retrying after {retry_after} seconds.\")\n","                    else:\n","                        raise RuntimeError(f\"API returned a non-200 status code: {response.status}\")\n","            except (ClientError, asyncio.TimeoutError) as e:\n","                logging.error(f\"Request failed due to network error: {e}\")\n","            # Wait before retrying with exponential backoff\n","            sleep_time = backoff_factor ** attempt\n","            logging.info(f\"Retrying in {sleep_time} seconds...\")\n","            await asyncio.sleep(sleep_time)\n","            attempt += 1\n","    # After all retries, raise an exception to indicate the request has ultimately failed\n","    raise RuntimeError(\"Request failed after multiple retries.\")\n","\n","\n","async def generate_text_async_hf(session, text, prompt, generation_params, semaphore):\n","    payload = {\n","        \"inputs\": prompt.format(text=text),\n","        \"parameters\": {**generation_params}\n","    }\n","    # Call the request_with_retry function to handle potential retries\n","    response_json = await request_with_retry_hf(session, API_URL, HEADERS, payload, semaphore)\n","    generated_text = response_json[0].get(\"generated_text\", \"No text generated\")\n","    if \"error\" in response_json:\n","        raise RuntimeError(f\"API returned an error: {response_json['error']}\")\n","    return generated_text\n","\n","\n","async def request_with_retry_oai(session, messages, generation_params, semaphore, retries=4, backoff_factor=3):\n","    \"\"\"Attempt a request to the OpenAI API with exponential backoff and retry logic.\"\"\"\n","    attempt = 0\n","    while attempt < retries:\n","        async with semaphore:\n","            try:\n","                completion = await client_oai.chat.completions.create(\n","                    model=MODEL,\n","                    messages=messages,\n","                    **generation_params\n","                )\n","                return completion.choices[0].message.content\n","            except openai.RateLimitError as e:\n","                retry_after = int(e.headers.get(\"Retry-After\", 60))\n","                logging.warning(f\"Rate limit exceeded. Retrying after {retry_after} seconds.\")\n","                await asyncio.sleep(retry_after)\n","            except (openai.APITimeoutError, asyncio.TimeoutError) as e:\n","                logging.error(f\"Request failed due to API or network error: {e}\")\n","                sleep_time = backoff_factor ** attempt\n","                logging.info(f\"Retrying in {sleep_time} seconds...\")\n","                await asyncio.sleep(sleep_time)\n","                attempt += 1\n","    # After all retries, raise an exception to indicate the request has ultimately failed\n","    raise RuntimeError(\"Request failed after multiple retries.\")\n","\n","\n","async def generate_text_async_oai(session, text, prompt, generation_params, semaphore):\n","    messages = [{\"role\": \"user\", \"content\": prompt.format(text=text)}]\n","    if \"max_new_tokens\" in generation_params:\n","        generation_params[\"max_tokens\"] = generation_params.pop(\"max_new_tokens\")\n","    allowed_params = {\"top_p\", \"temperature\", \"max_tokens\", \"stop\"}\n","    generation_params = {k: v for k, v in generation_params.items() if k in allowed_params}\n","\n","    # Call the request_with_retry_oai function to handle potential retries\n","    generated_text = await request_with_retry_oai(session, messages, generation_params, semaphore)\n","    return generated_text\n","\n","\n","async def run_batch(dataset, prompt, generation_params, api_provider, batch_size, sleep_time):\n","    results_lst = []\n","    semaphore = asyncio.BoundedSemaphore(128)\n","    timeout = ClientTimeout(total=60)\n","\n","    async with ClientSession(timeout=timeout) as session:\n","        for i in tqdm(range(0, len(dataset), batch_size), desc=\"Processing batches\"):\n","            text_batch = dataset[i:i + batch_size][\"sentence\"]\n","            if api_provider == \"HF\":\n","                tasks = [generate_text_async_hf(session, text, prompt, generation_params, semaphore) for text in text_batch]\n","            elif api_provider == \"OAI\":\n","                tasks = [generate_text_async_oai(session, text, prompt, generation_params, semaphore) for text in text_batch]\n","            else:\n","                raise ValueError(\"Invalid API provider\")\n","            results_batch = await asyncio.gather(*tasks)\n","            results_lst.extend(results_batch)\n","            await asyncio.sleep(sleep_time)\n","\n","    return results_lst\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"35fbf82d061e4f6ebf58b14e41583aee","version_major":2,"version_minor":0},"text/plain":["Processing batches:   0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2024-02-07 08:37:47,021 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:47,030 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:47,040 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:47,044 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:47,046 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:47,061 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:47,077 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:47,083 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:47,253 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:47,258 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:47,260 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:47,268 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:47,272 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"]},{"name":"stdout","output_type":"stream","text":["['neutral', 'negative', 'neutral']\n"]}],"source":["# run batch processing for simple prompt\n","\n","# run async function in jupyter notebook\n","#output_simple = await run_batch(dataset, prompt_financial_sentiment, generation_params, API_PROVIDER, BATCH_SIZE, SLEEP_TIME)\n","# run async function in .py script\n","output_simple = asyncio.run(run_batch(dataset, prompt_financial_sentiment, generation_params, API_PROVIDER, BATCH_SIZE, SLEEP_TIME))\n","\n","print(output_simple[:3])"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"30898d7e82d34392aff0b8dc8357e400","version_major":2,"version_minor":0},"text/plain":["Processing batches:   0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2024-02-07 08:37:54,774 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:54,815 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:54,819 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:54,844 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:54,941 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:54,972 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:55,030 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:55,034 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:55,039 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:55,043 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:55,150 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:55,222 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:55,258 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"36ddb70d076e41759639ed9d497d56b5","version_major":2,"version_minor":0},"text/plain":["Processing batches:   0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2024-02-07 08:37:56,789 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:56,809 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:56,813 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:56,858 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:56,878 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:56,906 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:56,913 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:56,964 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:56,968 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:57,089 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:57,104 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:57,189 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:57,193 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"487b5277d422473186968e0cf73ee232","version_major":2,"version_minor":0},"text/plain":["Processing batches:   0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2024-02-07 08:37:58,664 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:58,691 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:58,705 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:58,709 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:58,732 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:58,742 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:58,750 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:58,816 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:58,820 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:58,843 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:58,911 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:58,916 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-02-07 08:37:58,918 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"]},{"name":"stdout","output_type":"stream","text":["['{\"reason\": \"The text does not provide a clear indication of whether the benefits of transferring the Swedish business are positive or negative for investors\", \"label\": \"neutral\"}', '{\"reason\": \"A decrease in sales is generally considered negative for investors as it may indicate a decrease in revenue and potentially profitability\", \"label\": \"negative\"}', '{\"reason\": \"Lack of disclosure can be perceived negatively by investors as it may indicate lack of transparency or potentially negative news\", \"label\": \"negative\"}']\n"]}],"source":["# run batch processing for chain-of-thought and self-consistency prompt\n","output_cot_multiple = []\n","for _ in range(SELF_CONSISTENCY_ITERATIONS):\n","    # run async function in jupyter notebook\n","    #output_cot = await run_batch(dataset, prompt_financial_sentiment_cot, generation_params, API_PROVIDER, BATCH_SIZE, SLEEP_TIME)\n","    # run async function in .py script\n","    output_cot = asyncio.run(run_batch(dataset, prompt_financial_sentiment_cot, generation_params, API_PROVIDER, BATCH_SIZE, SLEEP_TIME))\n","    \n","    output_cot_multiple.append(output_cot)\n","\n","print(output_cot[:3])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Parsing failed for output: \n","{\"reason\": \"This text does not contain any information about the financial performance of the company, which is essential for making investment decisions. Therefore, it is impossible to classify this as positive, negative, or neutral. \", \"label\": \"neutral\"}\n","\n","TEXT: The company's revenue is expected to be in the range of EUR 6.5 - 6.8 billion in 2023.\n","JSON response: {\"reason\": \"The text does not give information whether the revenue is an improvement or decline from previous years, therefore a neutral sentiment is appropriate.\", \"label\": \"neutral, Error: unterminated string literal (detected at line 4) (<unknown>, line 4)\n"]}],"source":["# parse and clean outputs\n","random.seed(SEED)\n","\n","labels = [\"positive\", \"negative\", \"neutral\"]\n","\n","# function to map each label string to a discrete category (for simple prompt)\n","def clean_output(string, random_choice=True):\n","    for category in labels:\n","        if category.lower() in string.lower():\n","            return category\n","    if random_choice:\n","        return random.choice(labels)  # random category if no category is found\n","    else:\n","        return \"FAIL\"\n","\n","# function to parse chain-of-thought JSON output\n","def process_output_cot(output):\n","    try: \n","        output_dic = ast.literal_eval(output) \n","        return output_dic\n","    except Exception as e:\n","        # if json/dict parse fails, do simple search for occurance of first label term\n","        print(f\"Parsing failed for output: {output}, Error: {e}\")\n","        output_cl = clean_output(output, random_choice=False)\n","        output_dic = {\"reason\": \"FAIL\", \"label\": output_cl}\n","        return output_dic\n","\n","\n","# clean outputs for simple prompt\n","output_simple_cl = [clean_output(output) for output in output_simple]\n","\n","# clean outputs for CoT + SC prompt\n","output_cot_multiple_cl = []\n","output_dic_lst = []\n","for i in range(SELF_CONSISTENCY_ITERATIONS):\n","    output_dic_step = [process_output_cot(output) for output in output_cot_multiple[i]]\n","    output_labels = [dic[\"label\"] for dic in output_dic_step]\n","    output_cot_multiple_cl.append(output_labels)\n","    output_dic_lst.extend(output_dic_step)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sc_iter1</th>\n","      <th>sc_iter2</th>\n","      <th>sc_iter3</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>negative</td>\n","      <td>negative</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>positive</td>\n","      <td>positive</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>neutral</td>\n","      <td>neutral</td>\n","      <td>neutral</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   sc_iter1  sc_iter2  sc_iter3\n","0  negative  negative  negative\n","1  positive  positive  positive\n","2   neutral   neutral   neutral"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# convert the CoT+SC output to dataframe for easier downstream processing\n","df_output = pd.DataFrame(data=output_cot_multiple_cl).T\n","df_output = df_output.rename(columns={0: \"sc_iter1\", 1: \"sc_iter2\", 2: \"sc_iter3\"})\n","df_output.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sc_iter1</th>\n","      <th>sc_iter2</th>\n","      <th>sc_iter3</th>\n","      <th>label_llm_cot_multiple</th>\n","      <th>label_llm_cot</th>\n","      <th>label_llm</th>\n","      <th>label_experts</th>\n","      <th>text</th>\n","      <th>reason_iter1</th>\n","      <th>reason_iter2</th>\n","      <th>reason_iter3</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>negative</td>\n","      <td>negative</td>\n","      <td>negative</td>\n","      <td>negative</td>\n","      <td>negative</td>\n","      <td>neutral</td>\n","      <td>negative</td>\n","      <td>LONDON MarketWatch -- Share prices ended lower...</td>\n","      <td>The fact that the market ended lower indicates...</td>\n","      <td>Ending the day lower is negative for investors...</td>\n","      <td>The text mentions that share prices in London ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>positive</td>\n","      <td>positive</td>\n","      <td>positive</td>\n","      <td>positive</td>\n","      <td>positive</td>\n","      <td>positive</td>\n","      <td>positive</td>\n","      <td>At the same time profit of the company increas...</td>\n","      <td>An increase in profit is generally positive. I...</td>\n","      <td>An increase in profit is positive for investors</td>\n","      <td>The text mentions an increase in profits, whic...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>neutral</td>\n","      <td>neutral</td>\n","      <td>neutral</td>\n","      <td>neutral</td>\n","      <td>neutral</td>\n","      <td>neutral</td>\n","      <td>neutral</td>\n","      <td>The hull of the vessel was built one block at ...</td>\n","      <td>The text does not provide any information abou...</td>\n","      <td>This statement does not provide any direct fin...</td>\n","      <td>The text only mentions a construction process ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   sc_iter1  sc_iter2  sc_iter3 label_llm_cot_multiple label_llm_cot  \\\n","0  negative  negative  negative               negative      negative   \n","1  positive  positive  positive               positive      positive   \n","2   neutral   neutral   neutral                neutral       neutral   \n","\n","  label_llm label_experts                                               text  \\\n","0   neutral      negative  LONDON MarketWatch -- Share prices ended lower...   \n","1  positive      positive  At the same time profit of the company increas...   \n","2   neutral       neutral  The hull of the vessel was built one block at ...   \n","\n","                                        reason_iter1  \\\n","0  The fact that the market ended lower indicates...   \n","1  An increase in profit is generally positive. I...   \n","2  The text does not provide any information abou...   \n","\n","                                        reason_iter2  \\\n","0  Ending the day lower is negative for investors...   \n","1    An increase in profit is positive for investors   \n","2  This statement does not provide any direct fin...   \n","\n","                                        reason_iter3  \n","0  The text mentions that share prices in London ...  \n","1  The text mentions an increase in profits, whic...  \n","2  The text only mentions a construction process ...  "]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["# find majority from multiple self-consistency runs\n","from collections import Counter\n","random.seed(SEED)\n","\n","def find_majority(row):\n","    # Find majority\n","    count = Counter(row)\n","    majority = count.most_common(1)[0]\n","    # Check if it's a real majority or if all 3 labels appear 3 times\n","    if majority[1] > 1:\n","        return majority[0]\n","    else: # in case all 3 labels appear 3 times\n","        return random.choice(labels)\n","\n","\n","# majority for multiple self-consistency + CoT runs\n","df_output['label_llm_cot_multiple'] = df_output.apply(find_majority, axis=1)\n","# single CoT run\n","df_output[\"label_llm_cot\"] = [clean_output(output, random_choice=True) for output in df_output[\"sc_iter1\"]]  # if parsing did not work, choose a random label\n","# simple labeling prompt\n","df_output[\"label_llm\"] = output_simple_cl\n","# expert label\n","df_output[\"label_experts\"] = dataset[\"label_text\"]\n","\n","df_output[\"text\"] = dataset[\"sentence\"]\n","\n","# also add the reasoning for the first iteration for inspection\n","df_output[\"reason_iter1\"] = [dic[\"reason\"] for dic in output_dic_lst[:len(df_output)]]\n","df_output[\"reason_iter2\"] = [dic[\"reason\"] for dic in output_dic_lst[len(df_output):len(df_output)*2]]\n","df_output[\"reason_iter3\"] = [dic[\"reason\"] for dic in output_dic_lst[len(df_output)*2:len(df_output)*3]]\n","\n","df_output.head(3)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# save for debugging and in case something fails after API calls\n","df_output.to_csv(f'/home/ubuntu/data/df_{API_PROVIDER}_backup.csv', index=False)"]},{"cell_type":"markdown","metadata":{},"source":["#### Calculate metrics"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import balanced_accuracy_score, precision_recall_fscore_support, accuracy_score, classification_report\n","\n","def compute_metrics(label_experts, label_pred):\n","\n","    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(label_experts, label_pred, average='macro')  # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html\n","    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(label_experts, label_pred, average='micro')  # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html\n","    acc_balanced = balanced_accuracy_score(label_experts, label_pred)\n","    acc_not_balanced = accuracy_score(label_experts, label_pred)\n","\n","    metrics = {\n","        'f1_macro': f1_macro,\n","        'f1_micro': f1_micro,\n","        'accuracy_balanced': acc_balanced,\n","        'accuracy': acc_not_balanced,\n","        'precision_macro': precision_macro,\n","        'recall_macro': recall_macro,\n","        'precision_micro': precision_micro,\n","        'recall_micro': recall_micro,\n","    }\n","    metrics_report = classification_report(\n","        label_experts, label_pred, digits=2, output_dict=True, zero_division='warn'\n","    )\n","\n","    return {**metrics, **{\"report\": metrics_report}}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# compute metrics\n","if FINAL_TEST_RUN:\n","    if \"HF\" in API_PROVIDER:\n","        df_output_test = df_output.iloc[row_id_test]\n","        df_output_train = df_output.iloc[row_id_train]\n","    elif API_PROVIDER == \"OAI\":\n","        df_output_test = df_output\n","        df_output_train = pd.DataFrame()\n","    else:\n","        raise NotImplementedError\n","    print(f\"Length of testset: {len(df_output_test)}, and length of trainset: {len(df_output_train)}\")\n","else:\n","    df_output_test = df_output.copy(deep=True)\n","    df_output_train = pd.DataFrame()\n","\n","\n","label_experts = df_output_test[\"label_experts\"]\n","label_llm = df_output_test[\"label_llm\"]\n","label_llm_cot = df_output_test[\"label_llm_cot\"]  #[label if label in labels else random.choice(labels) for label in df_output[\"label_llm_cot\"]]\n","label_llm_cot_multiple = df_output_test[\"label_llm_cot_multiple\"]  #[label if label in labels else random.choice(labels) for label in df_output[\"label_llm_cot_multiple\"]] # replacing FAIL with a random label\n","\n","metrics_single = compute_metrics(label_experts, label_llm)\n","metrics_single_cot = compute_metrics(label_experts, label_llm_cot)\n","metrics_multiple_cot =  compute_metrics(label_experts, label_llm_cot_multiple)\n","\n","metrics = {\"metrics_single\": metrics_single, \"metrics_single_cot\": metrics_single_cot, \"metrics_multiple_cot\": metrics_multiple_cot}\n","\n","metrics"]},{"cell_type":"markdown","metadata":{},"source":["### Save results"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# save to disk\n","if SAVE_OUTPUTS: \n","    time_now = datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n","\n","    file_path_metrics = f'/home/ubuntu/data/metrics_{API_PROVIDER}_{time_now}_{MODEL}.json'\n","\n","    # Writing the metrics dictiontary to json\n","    with open(file_path_metrics, 'w') as file:\n","        json.dump(metrics, file, indent=4)\n","\n","    # Write dfs to csv\n","    df_output_train.to_csv(f'/home/ubuntu/data/df_train_{API_PROVIDER}_{time_now}_{MODEL}.csv', index=False)\n","    df_output_test.to_csv(f'/home/ubuntu/data/df_test_{API_PROVIDER}_{time_now}_{MODEL}.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# inspect the texts where the model and the dataset labels from the experts disagree\n","df_wrong = df_output[df_output[\"label_llm_cot_multiple\"] != df_output[\"label_experts\"]]\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOcwY6KN9Dpy7bY0RrBnU7G","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.1.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"03f56d49d1d24b43b1bf0690c71f63ee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0dbbb13307ed42faad32e517c2311362":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19b0e2d3008a437ba69025380b8d44f2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"294f43eb582544848ca58fffbb888e6c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c5746f523aa40c280fe50548f4199b1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"336d81a811cb486e80197c3c4a522174":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3588920f173f43a9a31a453e5797843a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0dbbb13307ed42faad32e517c2311362","placeholder":"​","style":"IPY_MODEL_bfecaf37ecce4f74ade13ebdaa042c64","value":"Resolving data files: 100%"}},"3c421b4bfd7a40fd9ecf77f3e87692cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7db6fe097c14a3e913e90733f1f9a7f","placeholder":"​","style":"IPY_MODEL_ff7bb0e11c19425abf8a4e43f9d02716","value":"Map: 100%"}},"65b6e4dab9574641b5e30179ea56907c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3c421b4bfd7a40fd9ecf77f3e87692cb","IPY_MODEL_a4618b08797c4586adbeaf576633fdb3","IPY_MODEL_6724fd0d7cc64db993ee163089b50186"],"layout":"IPY_MODEL_294f43eb582544848ca58fffbb888e6c"}},"6724fd0d7cc64db993ee163089b50186":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_951094de028e46f4b9fa3a157b562963","placeholder":"​","style":"IPY_MODEL_03f56d49d1d24b43b1bf0690c71f63ee","value":" 988/988 [00:00&lt;00:00, 8437.37 examples/s]"}},"7ce31cecc2814678ab858046bdfa3702":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"951094de028e46f4b9fa3a157b562963":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ea9225228bc481c8ecf3fab227117f4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3588920f173f43a9a31a453e5797843a","IPY_MODEL_d61c432471e048a7a57f631352a42af1","IPY_MODEL_d3bfcce0cef2461888e714ace975d149"],"layout":"IPY_MODEL_19b0e2d3008a437ba69025380b8d44f2"}},"a4618b08797c4586adbeaf576633fdb3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_336d81a811cb486e80197c3c4a522174","max":988,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d0c8ea12cb4641698a6ac36edbb742d2","value":988}},"bfecaf37ecce4f74ade13ebdaa042c64":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d0c8ea12cb4641698a6ac36edbb742d2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d3bfcce0cef2461888e714ace975d149":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ddbe259f9fff42cb9251b582e624d3a9","placeholder":"​","style":"IPY_MODEL_7ce31cecc2814678ab858046bdfa3702","value":" 5534/5534 [00:02&lt;00:00, 4270.68it/s]"}},"d61c432471e048a7a57f631352a42af1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f4a8592f5c9045e99367c532a88b05f2","max":5534,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2c5746f523aa40c280fe50548f4199b1","value":5534}},"ddbe259f9fff42cb9251b582e624d3a9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4a8592f5c9045e99367c532a88b05f2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7db6fe097c14a3e913e90733f1f9a7f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff7bb0e11c19425abf8a4e43f9d02716":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
