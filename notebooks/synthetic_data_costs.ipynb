{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install requirements\n",
    "#!pip install tiktoken\n",
    "# ... see other scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the API token\n",
    "import os\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "load_dotenv()\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "\n",
    "SEED = 42\n",
    "HEADERS = {\"Authorization\": f\"Bearer {hf_token}\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['sentence', 'label'],\n",
      "    num_rows: 2264\n",
      "})\n",
      "Dataset({\n",
      "    features: ['sentence', 'label'],\n",
      "    num_rows: 2000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# financial_phrasebank paper: https://arxiv.org/pdf/1307.5336.pdf\n",
    "random.seed(SEED)\n",
    "\n",
    "# load dataset\n",
    "dataset = load_dataset(\n",
    "    \"financial_phrasebank\", \"sentences_allagree\",  # \"sentences_66agree\", \"sentences_75agree\", \"sentences_allagree\"\n",
    "    split=\"train\"  # note that the dataset does not have a default test split\n",
    ")\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "# sampling down to from 2264 to 2000 texts to simplify calculations\n",
    "dataset_samp = dataset.select(random.sample(range(len(dataset)), 2000))\n",
    "\n",
    "print(dataset_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt is inspired by the annotator instructions provided in section \"Annotation task and instructions\"\n",
    "# in the financial_phrasebank paper: https://arxiv.org/pdf/1307.5336.pdf\n",
    "\n",
    "prompt_financial_sentiment = \"\"\"\\\n",
    "You are a highly qualified expert trained to annotate machine learning training data.\n",
    "\n",
    "Your task is to analyze the sentiment in the TEXT below from an investor perspective and label it with only one the three labels:\n",
    "positive, negative, or neutral.\n",
    "\n",
    "Base your label decision only on the TEXT and do not speculate e.g. based on prior knowledge about a company. \n",
    "\n",
    "Do not provide any explanations and only respond with one of the labels as one word: negative, positive, or neutral\n",
    "\n",
    "Examples:\n",
    "Text: Operating profit increased, from EUR 7m to 9m compared to the previous reporting period.\n",
    "Label: positive\n",
    "Text: The company generated net sales of 11.3 million euro this year.\n",
    "Label: neutral\n",
    "Text: Profit before taxes decreased to EUR 14m, compared to EUR 19m in the previous period.\t\n",
    "Label: negative\n",
    "\n",
    "Your TEXT to analyse:\n",
    "TEXT: {text}\n",
    "Label: \"\"\"\n",
    "\n",
    "\n",
    "prompt_financial_sentiment_cot = \"\"\"\\\n",
    "You are a highly qualified expert trained to annotate machine learning training data.\n",
    "\n",
    "Your task is to briefly analyze the sentiment in the TEXT below from an investor perspective and then label it with only one the three labels:\n",
    "positive, negative, neutral.\n",
    "\n",
    "Base your label decision only on the TEXT and do not speculate e.g. based on prior knowledge about a company. \n",
    "\n",
    "You first reason step by step about the correct label and then return your label.\n",
    "\n",
    "You ALWAYS respond only in the following JSON format: {{\"reason\": \"...\", \"label\": \"...\"}}\n",
    "You only respond with one single JSON response. \n",
    "\n",
    "Examples:\n",
    "Text: Operating profit increased, from EUR 7m to 9m compared to the previous reporting period.\n",
    "JSON response: {{\"reason\": \"An increase in operating profit is positive for investors\", \"label\": \"positive\"}}\n",
    "Text: The company generated net sales of 11.3 million euro this year.\n",
    "JSON response: {{\"reason\": \"The text only mentions financials without indication if they are better or worse than before\", \"label\": \"neutral\"}}\n",
    "Text: Profit before taxes decreased to EUR 14m, compared to EUR 19m in the previous period.\t\n",
    "JSON response: {{\"reason\": \"A decrease in profit is negative for investors\", \"label\": \"negative\"}}\n",
    "\n",
    "Your TEXT to analyse:\n",
    "TEXT: {text}\n",
    "JSON response: \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### per-time costs encoder-only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RoBERTa-base model on HF endpoints\n",
    "api_url_cpu = \"https://osz2sc9o618ptoc9.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    "api_url_t4_gpu = \"https://m3m487zksqezracf.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    "api_url_a10g_gpu = \"https://lxdr9wtekrtn9paf.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    "\n",
    "headers = {\n",
    "\t\"Accept\" : \"application/json\",\n",
    "\t\"Authorization\": f\"Bearer {hf_token}\",\n",
    "\t\"Content-Type\": \"application/json\" \n",
    "}\n",
    "\n",
    "def query(payload, api_url=None):\n",
    "    try:\n",
    "        response = requests.post(api_url, headers=headers, json=payload)\n",
    "        response.raise_for_status()  # This will raise an error for HTTP error codes\n",
    "        return response.json()\n",
    "    except requests.RequestException as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "def batch_query(batch, api_url=None):\n",
    "    responses = query(\n",
    "        {\"inputs\": batch['sentence'], \"parameters\": {}},\n",
    "        api_url=api_url,\n",
    "    )\n",
    "    return {\"label_pred\": responses}\n",
    "\n",
    "def measure_latency(batch, api_url=None):\n",
    "    start_time = time.perf_counter()\n",
    "    response = batch_query(batch, api_url) \n",
    "    end_time = time.perf_counter()\n",
    "    latency = end_time - start_time\n",
    "    # Assuming each item in the batch is processed with the same latency\n",
    "    return {\"latency\": [latency] * len(batch[\"sentence\"])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a36cd4e7ffe4b50bbfe44ff5b0a82fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency: 0.128 s, Throughput: 61.49 texts/s\n"
     ]
    }
   ],
   "source": [
    "# T4 GPU run\n",
    "start_time_t4_gpu = time.time()\n",
    "\n",
    "batch_size_t4_gpu = 8  # this influences throughput vs. latency\n",
    "dataset_t4_gpu = dataset_samp.map(lambda x: measure_latency(x, api_url=api_url_t4_gpu), batched=True, batch_size=batch_size_t4_gpu, load_from_cache_file=False)\n",
    "\n",
    "end_time_t4_gpu = time.time()\n",
    "\n",
    "# calculate latency and throughput\n",
    "latency_t4_gpu = np.mean(dataset_t4_gpu[\"latency\"])\n",
    "run_time_t4_gpu = end_time_t4_gpu - start_time_t4_gpu\n",
    "throughput_t4_gpu = len(dataset_t4_gpu) / run_time_t4_gpu\n",
    "print(f\"Latency: {latency_t4_gpu:.3f} s, Throughput: {throughput_t4_gpu:.2f} texts/s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d377037089a478395df22a9ea117bfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency: 0.106 s, Throughput: 73.63 texts/s\n"
     ]
    }
   ],
   "source": [
    "# A10G GPU run\n",
    "start_time_a10g_gpu = time.time()\n",
    "\n",
    "batch_size_a10g_gpu = 8  # this influences throughput vs. latency\n",
    "dataset_a10g_gpu = dataset_samp.map(lambda x: measure_latency(x, api_url=api_url_a10g_gpu), batched=True, batch_size=batch_size_a10g_gpu, load_from_cache_file=False)\n",
    "\n",
    "end_time_a10g_gpu = time.time()\n",
    "\n",
    "# calculate latency and throughput\n",
    "latency_a10g_gpu = np.mean(dataset_a10g_gpu[\"latency\"])\n",
    "run_time_a10g_gpu = end_time_a10g_gpu - start_time_a10g_gpu\n",
    "throughput_a10g_gpu = len(dataset_a10g_gpu) / run_time_a10g_gpu\n",
    "print(f\"Latency: {latency_a10g_gpu:.3f} s, Throughput: {throughput_a10g_gpu:.2f} texts/s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bd325b068ca48d1af191794526e1b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency: 0.875 s, Throughput: 4.56 texts/s\n"
     ]
    }
   ],
   "source": [
    "# CPU run\n",
    "start_time_cpu = time.time()\n",
    "\n",
    "batch_size_cpu = 4  # this influences throughput vs. latency\n",
    "dataset_cpu = dataset_samp.map(lambda x: measure_latency(x, api_url=api_url_cpu), batched=True, batch_size=batch_size_cpu, load_from_cache_file=False)\n",
    "\n",
    "end_time_cpu = time.time()\n",
    "\n",
    "# calculate latency and throughput\n",
    "latency_cpu = np.mean(dataset_cpu[\"latency\"])\n",
    "run_time_cpu = end_time_cpu - start_time_cpu\n",
    "throughput_cpu = len(dataset_cpu) / run_time_cpu\n",
    "print(f\"Latency: {latency_cpu:.3f} s, Throughput: {throughput_cpu:.2f} texts/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean time per datapoint: 0.0163 s\n",
      "Total time per 1000000 datapoints: 16261.86 s  /  271.03 min  /  4.52 h\n",
      "Throughput: 61.49 datapoints/s\n",
      "Latency: 0.128 s\n",
      "\n",
      "\n",
      "Mean time per datapoint: 0.0136 s\n",
      "Total time per 1000000 datapoints: 13581.47 s  /  226.36 min  /  3.77 h\n",
      "Throughput: 73.63 datapoints/s\n",
      "Latency: 0.106 s\n",
      "\n",
      "\n",
      "Mean time per datapoint: 0.2193 s\n",
      "Total time per 1000000 datapoints: 219305.44 s  /  3655.09 min  /  60.92 h\n",
      "Throughput: 4.56 datapoints/s\n",
      "Latency: 0.875 s\n",
      "\n",
      "\n",
      "Cost for 1000000 datapoints: $2.71 (T4 GPU)\n",
      "Cost for 1000000 datapoints: $4.90 (A10g GPU)\n",
      "Cost for 1000000 datapoints: $7.31 (Icelake 4GB CPU)\n"
     ]
    }
   ],
   "source": [
    "def compute_time(start_time, end_time, dataset, n_datapoints=100_000):\n",
    "    time_dataset = end_time - start_time\n",
    "    \n",
    "    sec_per_datapoint_mean = time_dataset / len(dataset)\n",
    "    \n",
    "    sec_per_n_datapoints = sec_per_datapoint_mean * n_datapoints\n",
    "    \n",
    "    throughput = len(dataset) / time_dataset\n",
    "    latency = np.mean(dataset[\"latency\"])\n",
    "    \n",
    "    print(f\"Mean time per datapoint: {sec_per_datapoint_mean:.4f} s\")\n",
    "    print(f\"Total time per {n_datapoints} datapoints: {sec_per_n_datapoints:.2f} s  /  {sec_per_n_datapoints / 60:.2f} min  /  {sec_per_n_datapoints / 3600:.2f} h\")\n",
    "    print(f\"Throughput: {throughput:.2f} datapoints/s\")\n",
    "    print(f\"Latency: {latency:.3f} s\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    return sec_per_n_datapoints\n",
    "\n",
    "\n",
    "n_datapoints = 1_000_000\n",
    "sec_per_n_datapoints_t4_gpu = compute_time(start_time_t4_gpu, end_time_t4_gpu, dataset_t4_gpu, n_datapoints=n_datapoints)\n",
    "sec_per_n_datapoints_a10g_gpu = compute_time(start_time_a10g_gpu, end_time_a10g_gpu, dataset_a10g_gpu, n_datapoints=n_datapoints)\n",
    "sec_per_n_datapoints_cpu = compute_time(start_time_cpu, end_time_cpu, dataset_cpu, n_datapoints=n_datapoints)\n",
    "\n",
    "# cost estimate\n",
    "#https://huggingface.co/pricing#endpoints\n",
    "cost_per_hour_t4_gpu = 0.60  # $/h\n",
    "cost_per_hour_a10g_gpu = 1.30  # $/h\n",
    "cost_per_hour_icelake4gb_cpu = 0.12  # $/h\n",
    "\n",
    "cost_per_n_datapoints_t4_gpu = cost_per_hour_t4_gpu * (sec_per_n_datapoints_t4_gpu / 3600)\n",
    "cost_per_n_datapoints_a10g_gpu = cost_per_hour_a10g_gpu * (sec_per_n_datapoints_a10g_gpu / 3600)\n",
    "cost_per_n_datapoints_icelake4gb_cpu  = cost_per_hour_icelake4gb_cpu * (sec_per_n_datapoints_cpu / 3600)\n",
    "\n",
    "print(f\"Cost for {n_datapoints} datapoints: ${cost_per_n_datapoints_t4_gpu:.2f} (T4 GPU)\")\n",
    "print(f\"Cost for {n_datapoints} datapoints: ${cost_per_n_datapoints_a10g_gpu:.2f} (A10g GPU)\")\n",
    "print(f\"Cost for {n_datapoints} datapoints: ${cost_per_n_datapoints_icelake4gb_cpu:.2f} (Icelake 4GB CPU)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### per-token costs OAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated total cost estimate for 1000000 texts: $153.08 (GPT-3.5)\n",
      "Estimated total cost estimate for 1000000 texts: $3061.61 (GPT-4)\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "n_datapoints = 1_000_000\n",
    "n_output_tokens_per_task = 3  # roughly for single word prompt/output. Much more for CoT output. \n",
    "\n",
    "# Initialize tokenizers\n",
    "tokenizer_gpt35 = tiktoken.encoding_for_model(\"gpt-3.5-turbo-1106\")  \n",
    "tokenizer_gpt4 = tiktoken.encoding_for_model(\"gpt-4-0125-preview\")\n",
    "\n",
    "def count_tokens(dataset, prompt_template, tokenizer):\n",
    "    total_tokens = 0\n",
    "    for text in dataset['sentence']:\n",
    "        prompt = prompt_template.format(text=text)\n",
    "        tokens = tokenizer.encode(prompt)\n",
    "        total_tokens += len(tokens)\n",
    "    return total_tokens\n",
    "\n",
    "def calculate_and_print_cost(total_tokens_input, n_datapoints, input_cost_usd_per_token, output_cost_usd_per_token, model_name):\n",
    "    dataset_tokens_per_text_mean = total_tokens_input / len(dataset_samp)\n",
    "    input_tokens_for_n_datapoints = dataset_tokens_per_text_mean * n_datapoints\n",
    "    output_tokens_for_n_datapoints = n_output_tokens_per_task * n_datapoints\n",
    "\n",
    "    input_token_cost = input_tokens_for_n_datapoints * input_cost_usd_per_token\n",
    "    output_token_cost = output_tokens_for_n_datapoints * output_cost_usd_per_token\n",
    "\n",
    "    total_cost = input_token_cost + output_token_cost\n",
    "\n",
    "    print(f\"Estimated total cost estimate for {n_datapoints} texts: ${total_cost:.2f} ({model_name})\")\n",
    "    #print(f\"Mean tokens per text: {dataset_tokens_per_text_mean:.2f} ({model_name})\")\n",
    "\n",
    "# Calculate total tokens input for each model\n",
    "total_tokens_input_gpt35 = count_tokens(dataset=dataset_samp, prompt_template=prompt_financial_sentiment_cot, tokenizer=tokenizer_gpt35)\n",
    "total_tokens_input_gpt4 = count_tokens(dataset=dataset_samp, prompt_template=prompt_financial_sentiment_cot, tokenizer=tokenizer_gpt4)\n",
    "\n",
    "# OAI pricing as of 11.02.2024, https://openai.com/pricing\n",
    "input_cost_usd_per_token_gpt35 = 0.0005 / 1000\n",
    "output_cost_usd_per_token_gpt35 = 0.0015 / 1000\n",
    "input_cost_usd_per_token_gpt4 = 0.01 / 1000\n",
    "output_cost_usd_per_token_gpt4 = 0.03 / 1000\n",
    "\n",
    "# Calculate and print costs for GPT-3.5\n",
    "calculate_and_print_cost(\n",
    "    total_tokens_input=total_tokens_input_gpt35, n_datapoints=n_datapoints, input_cost_usd_per_token=input_cost_usd_per_token_gpt35, \n",
    "    output_cost_usd_per_token=output_cost_usd_per_token_gpt35, model_name=\"GPT-3.5\"\n",
    ")\n",
    "\n",
    "# Calculate and print costs for GPT-4\n",
    "calculate_and_print_cost(\n",
    "    total_tokens_input=total_tokens_input_gpt4, n_datapoints=n_datapoints, input_cost_usd_per_token=input_cost_usd_per_token_gpt4, \n",
    "    output_cost_usd_per_token=output_cost_usd_per_token_gpt4, model_name=\"GPT-4\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environmental costs and CO2 emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GPT4 energy consumption calculation\n",
    "# These are very rough estimates given the lack of information about the model and it's infrastructure\n",
    "\n",
    "# Good guesstimate of energy cost per GPT4 query: 0.0017 to 0.0026 KWh, see https://towardsdatascience.com/chatgpts-energy-use-per-query-9383b8654487\n",
    "# Other less thorough estimtaes: https://towardsdatascience.com/chatgpts-electricity-consumption-7873483feac4,  https://lifestyle.livemint.com/news/big-story/ai-carbon-footprint-openai-chatgpt-water-google-microsoft-111697802189371.html\n",
    "# => with 1 million queries: 0,0017 * 1000000 = 1700 KWh  or 0.0026 * 1000000 = 2600 KWh\n",
    "\n",
    "# We use the EPA calculator to translate KWh to CO2 emissions: https://www.epa.gov/energy/greenhouse-gas-equivalencies-calculator\n",
    "# => 1700 KWh = 0.735 metric tons of CO2 equivalents  or  2600 KWh = 1.1 metric tons of CO2 equivalents\n",
    "# note that the actual CO2 emissions depend on the energy mix of the data center where the model is hosted\n",
    "\n",
    "## RoBERTa energy consumption calculation\n",
    "# Time estimate on one T4 GPU to process 1 million sentences: 16261.86 s  /  271.03 min  /  4.52 h   (see calculations above)\n",
    "# We use the ML CO2 Impact calculator to translate GPU runtime to CO2 emissions: https://mlco2.github.io/impact\n",
    "# Parameters: T4 GPU, 4.52 h, provider AWS, region US East (N. Virginia)\n",
    "# => result: ~0.12 kg CO2 equivalents  (interface bugs with 4.52 decimal value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Time and cost estimate with HF API*: Note that these calculations are only done for testing and do not really make sense, as the HF API is free and rate limit at the time of writing. One could do a time calculation for a dedicated inference endpoint to calculate costs, but this is beyond the scope of the blog post on synthetic data and efficient smaller models. With a dedicated inference endpoint the batch size, and therefore total throughput, should be significantly increased to fully utilize hardware and to save costs as scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from aiohttp import ClientSession, ClientTimeout, ClientError\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import logging\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "# params for API: https://huggingface.co/docs/api-inference/detailed_parameters#text-generation-task\n",
    "# alternative list for API: https://huggingface.github.io/text-generation-inference/#/Text%20Generation%20Inference/generate\n",
    "# params for endpoints: https://huggingface.co/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient\n",
    "generation_params = dict(\n",
    "    top_p=0.90,\n",
    "    top_k=None,\n",
    "    temperature=0.8,\n",
    "    repetition_penalty=1.03,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=128,\n",
    "    return_full_text=False,\n",
    "    #seed=SEED,\n",
    "    max_time=None, \n",
    "    stream=False,\n",
    "    details=False,\n",
    "    use_cache=False,\n",
    "    wait_for_model=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asynchronous functions for efficiently calling on LLM APIs with batching\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# functions for calling the HF API with retries, async and batch processing\n",
    "async def request_with_retry_hf(session, url, headers, json, semaphore, retries=4, backoff_factor=3):\n",
    "    \"\"\"Attempt a request with exponential backoff and retry logic.\"\"\"\n",
    "    attempt = 0\n",
    "    while attempt < retries:\n",
    "        async with semaphore:\n",
    "            try:\n",
    "                async with session.post(url, headers=headers, json=json) as response:\n",
    "                    if response.status in [200, 201]:\n",
    "                        return await response.json()\n",
    "                    elif response.status == 429:\n",
    "                        retry_after = int(response.headers.get(\"Retry-After\", 60))\n",
    "                        logging.warning(f\"Rate limit exceeded. Retrying after {retry_after} seconds.\")\n",
    "                    else:\n",
    "                        raise RuntimeError(f\"API returned a non-200 status code: {response.status}\")\n",
    "            except (ClientError, asyncio.TimeoutError) as e:\n",
    "                logging.error(f\"Request failed due to network error: {e}\")\n",
    "            # Wait before retrying with exponential backoff\n",
    "            sleep_time = backoff_factor ** attempt\n",
    "            logging.info(f\"Retrying in {sleep_time} seconds...\")\n",
    "            await asyncio.sleep(sleep_time)\n",
    "            attempt += 1\n",
    "    # After all retries, raise an exception to indicate the request has ultimately failed\n",
    "    raise RuntimeError(\"Request failed after multiple retries.\")\n",
    "\n",
    "\n",
    "async def generate_text_async_hf(session, text, prompt, generation_params, semaphore):\n",
    "    payload = {\n",
    "        \"inputs\": prompt.format(text=text),\n",
    "        \"parameters\": {**generation_params}\n",
    "    }\n",
    "    # Call the request_with_retry function to handle potential retries\n",
    "    response_json = await request_with_retry_hf(session, API_URL, HEADERS, payload, semaphore)\n",
    "    generated_text = response_json[0].get(\"generated_text\", \"No text generated\")\n",
    "    if \"error\" in response_json:\n",
    "        raise RuntimeError(f\"API returned an error: {response_json['error']}\")\n",
    "    return generated_text\n",
    "\n",
    "async def run_batch(dataset, prompt, generation_params, batch_size, sleep_time):\n",
    "    results_lst = []\n",
    "    semaphore = asyncio.BoundedSemaphore(128)\n",
    "    timeout = ClientTimeout(total=60)\n",
    "\n",
    "    async with ClientSession(timeout=timeout) as session:\n",
    "        for i in tqdm(range(0, len(dataset), batch_size), desc=\"Processing batches\"):\n",
    "            text_batch = dataset[i:i + batch_size][\"sentence\"]\n",
    "            tasks = [generate_text_async_hf(session, text, prompt, generation_params, semaphore) for text in text_batch]\n",
    "            results_batch = await asyncio.gather(*tasks)\n",
    "            results_lst.extend(results_batch)\n",
    "            await asyncio.sleep(sleep_time)\n",
    "\n",
    "    return results_lst\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsample for faster testing\n",
    "random.seed(42)\n",
    "sample_size_small = 200\n",
    "dataset_samp_small = dataset_samp.select(random.sample(range(len(dataset_samp)), sample_size_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "sleep_time = 1\n",
    "\n",
    "start_time_mixtral_api = time.time()\n",
    "\n",
    "output_simple = await run_batch(dataset_samp_small, prompt_financial_sentiment, generation_params, batch_size, sleep_time)\n",
    "print(output_simple[:3])\n",
    "\n",
    "end_time_mixtral_api = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_time(start_time, end_time, dataset, n_datapoints=100_000):\n",
    "    time_dataset = end_time - start_time\n",
    "    \n",
    "    sec_per_datapoint_mean = time_dataset / len(dataset)\n",
    "    \n",
    "    sec_per_n_datapoints = sec_per_datapoint_mean * n_datapoints\n",
    "    \n",
    "    throughput = len(dataset) / time_dataset\n",
    "    \n",
    "    print(f\"Mean time per datapoint: {sec_per_datapoint_mean:.4f} s\")\n",
    "    print(f\"Total time per {n_datapoints} datapoints: {sec_per_n_datapoints:.2f} s  /  {sec_per_n_datapoints / 60:.2f} min  /  {sec_per_n_datapoints / 3600:.2f} h\")\n",
    "    print(f\"Throughput roughly: {throughput:.2f} datapoints/s\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    return sec_per_n_datapoints\n",
    "\n",
    "\n",
    "n_datapoints = 1_000_000\n",
    "sec_per_n_datapoints_mixtral_api = compute_time(start_time_mixtral_api, end_time_mixtral_api, dataset_samp_small, n_datapoints=n_datapoints)\n",
    "\n",
    "# cost estimate, https://huggingface.co/pricing#endpoints\n",
    "cost_per_hour_2A100_gpu = 13.00  # $/h\n",
    "\n",
    "cost_per_n_datapoints_2A100_gpu = cost_per_hour_2A100_gpu * (sec_per_n_datapoints_mixtral_api / 3600)\n",
    "\n",
    "print(f\"Cost for {n_datapoints} datapoints: ${cost_per_n_datapoints_2A100_gpu:.2f} (2xA100 GPU)\")\n",
    "\n",
    "print(\"Note that these calculations do not really make sense, because the HF API is currently free and rate limited.\")\n",
    "print(\"Cost calculations would need to be done with a dedicated endpoint and a much higher batch size for full hardware utilization.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
